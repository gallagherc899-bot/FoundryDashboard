# -*- coding: utf-8 -*-
"""Copy of PreandPostSmoteValidation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qRCLt_T8FomPL3MuvXdqGqe7X3ZqXLBS
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE
import shap

# Load and clean dataset
df = pd.read_csv("anonymized_parts.csv")
df.columns = df.columns.str.strip().str.lower().str.replace(" ", "_").str.replace("(", "", regex=False).str.replace(")", "", regex=False)
df["week_ending"] = pd.to_datetime(df["week_ending"], errors="coerce")
df = df.dropna(subset=["part_id", "scrap%", "order_quantity", "piece_weight_lbs", "week_ending"])

# Create binary target
df["scrap_class"] = (df["scrap%"] > 5.0).astype(int)

# Encode features
features = ["order_quantity", "piece_weight_lbs", "part_id"]
X = df[features].copy()
y = df["scrap_class"]
X["part_id"] = LabelEncoder().fit_transform(X["part_id"])

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)

# Level 1: Baseline Validation
print("ðŸ”µ LEVEL 1: BASELINE VALIDATION")
models = {
    "Random Forest": RandomForestClassifier(random_state=42),
    "Logistic Regression": LogisticRegression(max_iter=1000)
}
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    print(f"\n{name} (Pre-SMOTE):")
    print(classification_report(y_test, y_pred))
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.title(f"{name} Confusion Matrix (Pre-SMOTE)")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

# Level 2: Bias & Balance Checks
print("\nðŸŸ¡ LEVEL 2: BIAS & BALANCE CHECKS")
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)
rf_post = RandomForestClassifier(random_state=42)
rf_post.fit(X_resampled, y_resampled)
y_pred_post = rf_post.predict(X_test)
print("Random Forest (Post-SMOTE):")
print(classification_report(y_test, y_pred_post))

def cost_eval(cm, fn_cost=100, fp_cost=20):
    tn, fp, fn, tp = cm.ravel()
    return fn * fn_cost + fp * fp_cost

cm_pre = confusion_matrix(y_test, models["Random Forest"].predict(X_test))
cm_post = confusion_matrix(y_test, y_pred_post)
print(f"Cost (Pre-SMOTE): ${cost_eval(cm_pre)}")
print(f"Cost (Post-SMOTE): ${cost_eval(cm_post)}")

# Level 3: Interpretability & Trust
print("\nðŸ”µ LEVEL 3: INTERPRETABILITY & TRUST")
explainer_pre = shap.TreeExplainer(models["Random Forest"])
shap_values_pre = explainer_pre.shap_values(X_test)
shap.summary_plot(shap_values_pre, X_test, plot_type="bar", show=False)
plt.title("SHAP Summary (Pre-SMOTE)")
plt.show()

explainer_post = shap.TreeExplainer(rf_post)
shap_values_post = explainer_post.shap_values(X_test)
shap.summary_plot(shap_values_post, X_test, plot_type="bar", show=False)
plt.title("SHAP Summary (Post-SMOTE)")
plt.show()

# SHAP interaction plot fix
print("Feature names:", X_test.columns.tolist())
X_test_reset = X_test.reset_index(drop=True)
shap_inter_values = explainer_post.shap_interaction_values(X_test_reset)
shap.summary_plot(shap_inter_values[:, :, :, 1], X_test_reset)
plt.title("SHAP Interaction Summary (Post-SMOTE)")
plt.show()

# Level 4: Robustness & Generalization
print("\nðŸŸ£ LEVEL 4: ROBUSTNESS & GENERALIZATION")
for t in range(3, 8):
    df["scrap_class"] = (df["scrap%"] > t).astype(int)
    X = df[features]
    y = df["scrap_class"]
    X["part_id"] = LabelEncoder().fit_transform(X["part_id"])
    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)
    X_resampled, y_resampled = SMOTE(random_state=42).fit_resample(X_train, y_train)
    model = RandomForestClassifier(random_state=42)
    model.fit(X_resampled, y_resampled)
    y_pred = model.predict(X_test)
    print(f"\nThreshold: {t}%")
    print(classification_report(y_test, y_pred))

X = df[features]
y = df["scrap_class"]
X["part_id"] = LabelEncoder().fit_transform(X["part_id"])
X_train, _, y_train, _ = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)
X_resampled, y_resampled = SMOTE(random_state=42).fit_resample(X_train, y_train)
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(RandomForestClassifier(random_state=42), X_resampled, y_resampled, cv=cv, scoring="f1")
print("\n5-Fold Cross-Validation F1 Scores:", scores)
print("Mean F1:", np.mean(scores))

split_date = df["week_ending"].quantile(0.6)
train_df = df[df["week_ending"] <= split_date]
test_df = df[df["week_ending"] > split_date]
X_train = train_df[features]
y_train = train_df["scrap_class"]
X_test = test_df[features]
y_test = test_df["scrap_class"]
X_train["part_id"] = LabelEncoder().fit_transform(X_train["part_id"])
X_test["part_id"] = LabelEncoder().fit_transform(X_test["part_id"])
X_resampled, y_resampled = SMOTE(random_state=42).fit_resample(X_train, y_train)
model = RandomForestClassifier(random_state=42)
model.fit(X_resampled, y_resampled)
y_pred = model.predict(X_test)
print("\nTemporal Validation:")
print(classification_report(y_test, y_pred))

# Level 5: External Benchmarking
print("\nðŸ”´ LEVEL 5: EXTERNAL BENCHMARKING")
models = {
    "Random Forest": RandomForestClassifier(random_state=42),
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Gradient Boosting": GradientBoostingClassifier(random_state=42)
}
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)
X_resampled, y_resampled = SMOTE(random_state=42).fit_resample(X_train, y_train)
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    print(f"\n{name} (Pre-SMOTE):")
    print(classification_report(y_test, y_pred))
    model.fit(X_resampled, y_resampled)
    y_pred = model.predict(X_test)
    print(f"{name} (Post-SMOTE):")
    print(classification_report(y_test, y_pred))
